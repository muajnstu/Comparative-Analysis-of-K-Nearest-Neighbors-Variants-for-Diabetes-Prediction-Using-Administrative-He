{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muajnstu/DSK-Chain-to-predict-diabeties-/blob/main/Diabetics_Prediction_on_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnrk2cdTzeSd"
      },
      "source": [
        "# Primary data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yh9ZHxCtI33u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, f1_score)\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, f1_score, roc_auc_score, recall_score, precision_score)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import neighbors\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.cluster import KMeans\n",
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('https://raw.githubusercontent.com/muajnstu/Comparative-Analysis-of-K-Nearest-Neighbors-Variants-for-Diabetes-Prediction-Using-Administrative-He/refs/heads/main/raw%20data.csv')\n",
        "X = df.drop(columns=['Outcome'])\n",
        "y = df['Outcome']"
      ],
      "metadata": {
        "id": "bob80fE-oyXU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "E30vIJlbh255",
        "outputId": "38087324-ccc1-4dbd-8b9d-0254a1be22d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14222, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts().plot(kind='bar')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0, 1], labels=['0', '1'], rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "2KqTVX0Xoz2r",
        "outputId": "f4e0ef81-602f-47fc-9db9-4f719b981569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkHb5XozJH8d"
      },
      "outputs": [],
      "source": [
        "# --- Custom Hybrid KNN+SVM (with memberships) ---\n",
        "class KNNSVM(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X_np, y)\n",
        "        self._check_params(X_np, y)\n",
        "        self.X = X_np\n",
        "        self.y = y\n",
        "        self.xdim = self.X.shape[1]\n",
        "        self.n = len(y)\n",
        "        self.classes = np.unique(y)\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = self.y.values\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(self.X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = self.y.iloc[j]\n",
        "                one_train = self.X[j]\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = LinearSVC()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        # Convert r to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(r, pd.DataFrame):\n",
        "            r_np = r.values\n",
        "        else:\n",
        "            r_np = r\n",
        "\n",
        "        if len(set(self.label)) == 1:\n",
        "            return np.full(r_np.shape[0], self.label[0]) # Return array of the single class\n",
        "        return self.clf.predict(r_np)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # LinearSVC does not have predict_proba; use decision_function instead\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        if hasattr(self.clf, \"decision_function\"):\n",
        "            decision = self.clf.decision_function(X_np)\n",
        "            # Normalize to (0,1) range for ROC AUC, shape (n_samples, n_classes)\n",
        "            if decision.ndim == 1:\n",
        "                # Binary case\n",
        "                min_val, max_val = decision.min(), decision.max()\n",
        "                if min_val == max_val:\n",
        "                    probs = np.ones((len(decision), 2)) * 0.5\n",
        "                else:\n",
        "                    probs = np.zeros((len(decision), 2))\n",
        "                    probs[:, 1] = (decision - min_val) / (max_val - min_val)\n",
        "                    probs[:, 0] = 1 - probs[:, 1]\n",
        "                return probs\n",
        "            else:\n",
        "                # Multiclass case\n",
        "                exp_decision = np.exp(decision)\n",
        "                probs = exp_decision / exp_decision.sum(axis=1, keepdims=True)\n",
        "                return probs\n",
        "        else:\n",
        "            n = X_np.shape[0]\n",
        "            # Return uniform probabilities if decision_function is not available\n",
        "            return np.ones((n, len(self.classes))) / len(self.classes)\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(len(df))] # Use len(df) instead of self.n\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = self.y.iloc[i] # Access using integer position\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= X.shape[0]: # Check against number of samples\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "\n",
        "# --- Custom Hybrid KNN+Bayes (with memberships) ---\n",
        "class KNNBayes(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X_np, y)\n",
        "        self._check_params(X_np, y) # Pass numpy array to check_params\n",
        "        self.X = X_np\n",
        "        self.y = y\n",
        "        self.xdim = self.X.shape[1] # Use shape for dimensions\n",
        "        self.n = len(y)\n",
        "        self.classes = np.unique(y) # Get classes from y\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = self.y.values # Use .values to get numpy array for alignment\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(self.X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = self.y.iloc[j] # Access using integer position\n",
        "                one_train = self.X[j] # Access using numpy indexing\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = GaussianNB()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        # Convert r to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(r, pd.DataFrame):\n",
        "            r_np = r.values\n",
        "        else:\n",
        "            r_np = r\n",
        "\n",
        "        if len(set(self.label)) == 1:\n",
        "             return np.full(r_np.shape[0], self.label[0]) # Return array of the single class\n",
        "\n",
        "        return self.clf.predict(r_np)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        return self.clf.predict_proba(X_np)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(len(df))] # Use len(df) instead of self.n\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = self.y.iloc[i] # Access using integer position\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= X.shape[0]: # Check against number of samples\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "\n",
        "class KmeansKNN():\n",
        "    def __init__(self, n_neighbors=3, output='add', n_jobs=None, random_state=0):\n",
        "        self.output = output\n",
        "        self._random_state = random_state\n",
        "        self._cluster = None\n",
        "        self._kclass = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=n_jobs)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        if type(X_train) != np.ndarray:\n",
        "            X_train = X_train.values\n",
        "        self._cluster = KMeans(n_clusters=len(np.unique(y_train)), random_state=self._random_state, n_init=10).fit(X_train) # Added n_init\n",
        "        y_labels_train = self._cluster.labels_\n",
        "        if self.output == 'add':\n",
        "            X_train = np.append(X_train, np.reshape(y_labels_train, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_train = y_labels_train[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        self._kclass.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        if type(X_test) != np.ndarray:\n",
        "            X_test = X_test.values\n",
        "        y_labels_test = self._cluster.predict(X_test)\n",
        "        if self.output == 'add':\n",
        "            X_test = np.append(X_test, np.reshape(y_labels_test, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_test = y_labels_test[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        return self._kclass.predict(X_test)\n",
        "\n",
        "    def predict_proba(self, X_test):\n",
        "        if type(X_test) != np.ndarray:\n",
        "            X_test = X_test.values\n",
        "        y_labels_test = self._cluster.predict(X_test)\n",
        "        if self.output == 'add':\n",
        "            X_test = np.append(X_test, np.reshape(y_labels_test, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_test = y_labels_test[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        return self._kclass.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXQ2H3nxJx0m"
      },
      "outputs": [],
      "source": [
        "def print_metrics(y_true, y_pred, y_prob=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    num_classes = cm.shape[0]\n",
        "\n",
        "    if num_classes == 2:\n",
        "        # Binary classification\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        gmean = np.sqrt(specificity * sensitivity)\n",
        "        type1 = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        type2 = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        fmeasure = f1_score(y_true, y_pred, pos_label=1)\n",
        "        auc = 0\n",
        "        if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "            try:\n",
        "                auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "            except Exception:\n",
        "                auc = 0\n",
        "\n",
        "    else:\n",
        "        # Multiclass classification\n",
        "        TP = np.diag(cm)\n",
        "        FP = np.sum(cm, axis=0) - TP\n",
        "        FN = np.sum(cm, axis=1) - TP\n",
        "        TN = np.sum(cm) - (FP + FN + TP)\n",
        "\n",
        "        specificity = np.mean([\n",
        "            TN[i] / (TN[i] + FP[i]) if (TN[i] + FP[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        sensitivity = np.mean([\n",
        "            TP[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        gmean = np.sqrt(specificity * sensitivity)\n",
        "        type1 = np.mean([\n",
        "            FP[i] / (FP[i] + TN[i]) if (FP[i] + TN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        type2 = np.mean([\n",
        "            FN[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        fmeasure = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        auc = 0\n",
        "        if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "            try:\n",
        "                auc = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')\n",
        "            except Exception:\n",
        "                auc = 0\n",
        "\n",
        "    # Print or return results\n",
        "    print(f\"Accuracy      : {accuracy:.4f}\")\n",
        "    print(f\"Sensitivity   : {sensitivity:.4f}\")\n",
        "    print(f\"Specificity   : {specificity:.4f}\")\n",
        "    print(f\"G-Mean        : {gmean:.4f}\")\n",
        "    print(f\"Type I Error  : {type1:.4f}\")\n",
        "    print(f\"Type II Error : {type2:.4f}\")\n",
        "    print(f\"F1 Score      : {fmeasure:.4f}\")\n",
        "    print(f\"AUROC         : {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZDC1XFkJScb"
      },
      "outputs": [],
      "source": [
        "# --- KNN Variant Models ---\n",
        "def run_knn_variant(name, knn_clf):\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    if hasattr(knn_clf, \"predict_proba\"):\n",
        "        try:\n",
        "            y_prob = knn_clf.predict_proba(X_test)\n",
        "        except Exception:\n",
        "            y_prob = None\n",
        "    else:\n",
        "        y_prob = None\n",
        "    print_metrics(y_test, y_pred, y_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKAx8jZjLg3E"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('https://raw.githubusercontent.com/muajnstu/Comparative-Analysis-of-K-Nearest-Neighbors-Variants-for-Diabetes-Prediction-Using-Administrative-He/refs/heads/main/raw%20data.csv')\n",
        "X = df.drop(columns=['Outcome'])\n",
        "y = df['Outcome']\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46, stratify=y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PysVYKXh5V3D",
        "outputId": "74b5df73-53d2-494f-a0dd-143e1afce80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== KNN ====\n",
            "Accuracy      : 0.8155\n",
            "Sensitivity   : 0.1104\n",
            "Specificity   : 0.9459\n",
            "G-Mean        : 0.3231\n",
            "Type I Error  : 0.0541\n",
            "Type II Error : 0.8896\n",
            "F1 Score      : 0.1573\n",
            "AUROC         : 0.5516\n",
            "\n",
            "==== DistanceKNN ====\n",
            "Accuracy      : 0.8077\n",
            "Sensitivity   : 0.1081\n",
            "Specificity   : 0.9371\n",
            "G-Mean        : 0.3183\n",
            "Type I Error  : 0.0629\n",
            "Type II Error : 0.8919\n",
            "F1 Score      : 0.1493\n",
            "AUROC         : 0.5420\n",
            "\n",
            "==== GeneralizedKNN ====\n",
            "Accuracy      : 0.8011\n",
            "Sensitivity   : 0.1374\n",
            "Specificity   : 0.9238\n",
            "G-Mean        : 0.3563\n",
            "Type I Error  : 0.0762\n",
            "Type II Error : 0.8626\n",
            "F1 Score      : 0.1773\n",
            "AUROC         : 0.5585\n",
            "\n",
            "==== EuclideanKNN ====\n",
            "Accuracy      : 0.8155\n",
            "Sensitivity   : 0.1104\n",
            "Specificity   : 0.9459\n",
            "G-Mean        : 0.3231\n",
            "Type I Error  : 0.0541\n",
            "Type II Error : 0.8896\n",
            "F1 Score      : 0.1573\n",
            "AUROC         : 0.5516\n",
            "\n",
            "==== ManhattanKNN ====\n",
            "Accuracy      : 0.8165\n",
            "Sensitivity   : 0.1059\n",
            "Specificity   : 0.9479\n",
            "G-Mean        : 0.3168\n",
            "Type I Error  : 0.0521\n",
            "Type II Error : 0.8941\n",
            "F1 Score      : 0.1526\n",
            "AUROC         : 0.5652\n",
            "\n",
            "==== ChebyshevKNN ====\n",
            "Accuracy      : 0.8102\n",
            "Sensitivity   : 0.1081\n",
            "Specificity   : 0.9400\n",
            "G-Mean        : 0.3188\n",
            "Type I Error  : 0.0600\n",
            "Type II Error : 0.8919\n",
            "F1 Score      : 0.1509\n",
            "AUROC         : 0.5399\n",
            "\n",
            "==== MahalanobisKNN ====\n",
            "Accuracy      : 0.7747\n",
            "Sensitivity   : 0.1554\n",
            "Specificity   : 0.8892\n",
            "G-Mean        : 0.3717\n",
            "Type I Error  : 0.1108\n",
            "Type II Error : 0.8446\n",
            "F1 Score      : 0.1772\n",
            "AUROC         : 0.5542\n",
            "\n",
            "==== SeuclideanKNN ====\n",
            "Accuracy      : 0.8046\n",
            "Sensitivity   : 0.1284\n",
            "Specificity   : 0.9296\n",
            "G-Mean        : 0.3455\n",
            "Type I Error  : 0.0704\n",
            "Type II Error : 0.8716\n",
            "F1 Score      : 0.1701\n",
            "AUROC         : 0.5536\n",
            "\n",
            "==== WminkowskiKNN ====\n",
            "Accuracy      : 0.8011\n",
            "Sensitivity   : 0.1374\n",
            "Specificity   : 0.9238\n",
            "G-Mean        : 0.3563\n",
            "Type I Error  : 0.0762\n",
            "Type II Error : 0.8626\n",
            "F1 Score      : 0.1773\n",
            "AUROC         : 0.5585\n",
            "\n",
            "Project complete! Check the above output for performance of each KNN variant.\n"
          ]
        }
      ],
      "source": [
        "# Recalculate covariance and variance with the current X_train after SMOTE\n",
        "covariance_matrix = np.cov(X_train.T)\n",
        "stabilized_covariance_matrix = covariance_matrix + np.eye(covariance_matrix.shape[0]) * 1e-6\n",
        "inv_covariance_matrix = np.linalg.inv(stabilized_covariance_matrix)\n",
        "variance_vector = np.var(X_train, axis=0)\n",
        "\n",
        "knn_variants = {\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"DistanceKNN\": KNeighborsClassifier(n_neighbors=3, weights='distance'),\n",
        "    \"GeneralizedKNN\": KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3),\n",
        "    \"EuclideanKNN\": KNeighborsClassifier(n_neighbors=3, metric='euclidean'),\n",
        "    \"ManhattanKNN\": KNeighborsClassifier(n_neighbors=3, metric='manhattan'),\n",
        "    \"ChebyshevKNN\": KNeighborsClassifier(n_neighbors=3, metric='chebyshev'),\n",
        "    # Use the recalculated inverse covariance matrix and variance vector\n",
        "    \"MahalanobisKNN\": KNeighborsClassifier(n_neighbors=3, metric='mahalanobis', metric_params={'VI': inv_covariance_matrix}),\n",
        "    \"SeuclideanKNN\": KNeighborsClassifier(n_neighbors=3, metric='seuclidean', metric_params={'V': variance_vector}),\n",
        "    \"WminkowskiKNN\": KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3, metric_params={'w': np.ones(X_train.shape[1])}),\n",
        "    #\"KNNBayes\": KNNBayes(k=3, plot=False),\n",
        "    #\"KNNSVM\": KNNSVM(k=3, plot=False)\n",
        "}\n",
        "\n",
        "\n",
        "for name, model in knn_variants.items():\n",
        "    run_knn_variant(name, model)\n",
        "\n",
        "#print(\"\\n==== KMeansKNN ====\")\n",
        "#kmeansknn = KmeansKNN(n_neighbors=3, output='add')\n",
        "#kmeansknn.fit(X_train, y_train)\n",
        "#y_pred = kmeansknn.predict(X_test)\n",
        "#y_prob = kmeansknn.predict_proba(X_test)\n",
        "#print_metrics(y_test, y_pred, y_prob)\n",
        "\n",
        "print(\"\\nProject complete! Check the above output for performance of each KNN variant.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}