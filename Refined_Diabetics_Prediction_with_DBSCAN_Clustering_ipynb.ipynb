{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muajnstu/Comparative-Analysis-of-KNN-Variants-for-Diabetes-Prediction-Using-Administrative-Health-data/blob/main/Refined_Diabetics_Prediction_with_DBSCAN_Clustering_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh9ZHxCtI33u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, f1_score)\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, f1_score, roc_auc_score, recall_score, precision_score)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import neighbors\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.cluster import KMeans\n",
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkHb5XozJH8d"
      },
      "outputs": [],
      "source": [
        "# --- Custom Hybrid KNN+SVM (with memberships) ---\n",
        "class KNNSVM(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X_np, y)\n",
        "        self._check_params(X_np, y) # Pass numpy array to check_params\n",
        "        self.X = X_np\n",
        "        self.y = y\n",
        "        self.xdim = self.X.shape[1] # Use shape for dimensions\n",
        "        self.n = len(y)\n",
        "        self.classes = np.unique(y) # Get classes from y\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = self.y.values # Use .values to get numpy array for alignment\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(self.X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = self.y.iloc[j] # Access using integer position\n",
        "                one_train = self.X[j] # Access using numpy indexing\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = LinearSVC()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        # Convert r to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(r, pd.DataFrame):\n",
        "            r_np = r.values\n",
        "        else:\n",
        "            r_np = r\n",
        "\n",
        "        if len(set(self.label)) == 1:\n",
        "            return np.full(r_np.shape[0], self.label[0]) # Return array of the single class\n",
        "        return self.clf.predict(r_np)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # LinearSVC does not have predict_proba; use decision_function instead\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        if hasattr(self.clf, \"decision_function\"):\n",
        "            decision = self.clf.decision_function(X_np)\n",
        "            # Normalize to (0,1) range for ROC AUC, shape (n_samples, n_classes)\n",
        "            if decision.ndim == 1:\n",
        "                # Binary case\n",
        "                min_val, max_val = decision.min(), decision.max()\n",
        "                if min_val == max_val:\n",
        "                    probs = np.ones((len(decision), 2)) * 0.5\n",
        "                else:\n",
        "                    probs = np.zeros((len(decision), 2))\n",
        "                    probs[:, 1] = (decision - min_val) / (max_val - min_val)\n",
        "                    probs[:, 0] = 1 - probs[:, 1]\n",
        "                return probs\n",
        "            else:\n",
        "                # Multiclass case\n",
        "                exp_decision = np.exp(decision)\n",
        "                probs = exp_decision / exp_decision.sum(axis=1, keepdims=True)\n",
        "                return probs\n",
        "        else:\n",
        "            n = X_np.shape[0]\n",
        "            # Return uniform probabilities if decision_function is not available\n",
        "            return np.ones((n, len(self.classes))) / len(self.classes)\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(len(df))] # Use len(df) instead of self.n\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = self.y.iloc[i] # Access using integer position\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= X.shape[0]: # Check against number of samples\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "\n",
        "# --- Custom Hybrid KNN+Bayes (with memberships) ---\n",
        "class KNNBayes(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3, plot=True):\n",
        "        self.k = k\n",
        "        self.plot = plot\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        self.neigh = neighbors.NearestNeighbors(n_neighbors=14)\n",
        "        self.neigh.fit(X_np, y)\n",
        "        self._check_params(X_np, y) # Pass numpy array to check_params\n",
        "        self.X = X_np\n",
        "        self.y = y\n",
        "        self.xdim = self.X.shape[1] # Use shape for dimensions\n",
        "        self.n = len(y)\n",
        "        self.classes = np.unique(y) # Get classes from y\n",
        "        self.df = pd.DataFrame(self.X)\n",
        "        self.df['y'] = self.y.values # Use .values to get numpy array for alignment\n",
        "        self.memberships = self._compute_memberships()\n",
        "        self.df['membership'] = self.memberships\n",
        "        self.result = self.neigh.kneighbors(self.X)\n",
        "        self.label_index = self.result[1]\n",
        "        self.label = []\n",
        "        self.train = []\n",
        "        for i in self.label_index:\n",
        "            for j in i:\n",
        "                one_label = self.y.iloc[j] # Access using integer position\n",
        "                one_train = self.X[j] # Access using numpy indexing\n",
        "                self.label.append(one_label)\n",
        "                self.train.append(one_train)\n",
        "        self.np_label = np.array(self.label)\n",
        "        self.np_train = np.array(self.train)\n",
        "        self.clf = GaussianNB()\n",
        "        self.clf.fit(self.np_train, self.np_label)\n",
        "        self.fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, r):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict() called before fit()')\n",
        "        # Convert r to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(r, pd.DataFrame):\n",
        "            r_np = r.values\n",
        "        else:\n",
        "            r_np = r\n",
        "\n",
        "        if len(set(self.label)) == 1:\n",
        "             return np.full(r_np.shape[0], self.label[0]) # Return array of the single class\n",
        "\n",
        "        return self.clf.predict(r_np)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('predict_proba() called before fit()')\n",
        "        # Convert X to numpy array if it's a pandas DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X_np = X.values\n",
        "        else:\n",
        "            X_np = X\n",
        "\n",
        "        return self.clf.predict_proba(X_np)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        if not hasattr(self, \"fitted_\") or not self.fitted_:\n",
        "            raise Exception('score() called before fit()')\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.asarray(predictions)\n",
        "        return accuracy_score(y_pred=predictions, y_true=y)\n",
        "\n",
        "    def _find_k_nearest_neighbors(self, df, x):\n",
        "        X = df.iloc[:, 0:self.xdim].values\n",
        "        df['distances'] = [np.linalg.norm(X[i] - x) for i in range(len(df))] # Use len(df) instead of self.n\n",
        "        df.sort_values(by='distances', ascending=True, inplace=True)\n",
        "        neighbors = df.iloc[0:self.k]\n",
        "        return neighbors\n",
        "\n",
        "    def _get_counts(self, neighbors):\n",
        "        groups = neighbors.groupby('y')\n",
        "        counts = {group[1]['y'].iloc[0]: group[1].count()[0] for group in groups}\n",
        "        return counts\n",
        "\n",
        "    def _compute_memberships(self):\n",
        "        memberships = []\n",
        "        for i in range(self.n):\n",
        "            x = self.X[i]\n",
        "            y = self.y.iloc[i] # Access using integer position\n",
        "            neighbors = self._find_k_nearest_neighbors(pd.DataFrame.copy(self.df), x)\n",
        "            counts = self._get_counts(neighbors)\n",
        "            membership = dict()\n",
        "            for c in self.classes:\n",
        "                uci = 0.49 * (counts.get(c, 0) / self.k)\n",
        "                if c == y:\n",
        "                    uci += 0.51\n",
        "                membership[c] = uci\n",
        "            memberships.append(membership)\n",
        "        return memberships\n",
        "\n",
        "    def _check_params(self, X, y):\n",
        "        if type(self.k) != int:\n",
        "            raise Exception('\"k\" should have type int')\n",
        "        elif self.k >= X.shape[0]: # Check against number of samples\n",
        "            raise Exception('\"k\" should be less than no of feature sets')\n",
        "        elif self.k % 2 == 0:\n",
        "            raise Exception('\"k\" should be odd')\n",
        "        if type(self.plot) != bool:\n",
        "            raise Exception('\"plot\" should have type bool')\n",
        "\n",
        "\n",
        "class KmeansKNN():\n",
        "    def __init__(self, n_neighbors=3, output='add', n_jobs=None, random_state=0):\n",
        "        self.output = output\n",
        "        self._random_state = random_state\n",
        "        self._cluster = None\n",
        "        self._kclass = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=n_jobs)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        if type(X_train) != np.ndarray:\n",
        "            X_train = X_train.values\n",
        "        self._cluster = KMeans(n_clusters=len(np.unique(y_train)), random_state=self._random_state, n_init=10).fit(X_train) # Added n_init\n",
        "        y_labels_train = self._cluster.labels_\n",
        "        if self.output == 'add':\n",
        "            X_train = np.append(X_train, np.reshape(y_labels_train, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_train = y_labels_train[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        self._kclass.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        if type(X_test) != np.ndarray:\n",
        "            X_test = X_test.values\n",
        "        y_labels_test = self._cluster.predict(X_test)\n",
        "        if self.output == 'add':\n",
        "            X_test = np.append(X_test, np.reshape(y_labels_test, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_test = y_labels_test[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        return self._kclass.predict(X_test)\n",
        "\n",
        "    def predict_proba(self, X_test):\n",
        "        if type(X_test) != np.ndarray:\n",
        "            X_test = X_test.values\n",
        "        y_labels_test = self._cluster.predict(X_test)\n",
        "        if self.output == 'add':\n",
        "            X_test = np.append(X_test, np.reshape(y_labels_test, (-1, 1)), axis=1)\n",
        "        elif self.output == 'replace':\n",
        "            X_test = y_labels_test[:, np.newaxis]\n",
        "        else:\n",
        "            raise ValueError('output should be either add or replace')\n",
        "        return self._kclass.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXQ2H3nxJx0m"
      },
      "outputs": [],
      "source": [
        "def print_metrics(y_true, y_pred, y_prob=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    num_classes = cm.shape[0]\n",
        "\n",
        "    if num_classes == 2:\n",
        "        # Binary classification\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        gmean = np.sqrt(specificity * sensitivity)\n",
        "        type1 = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "        type2 = FN / (TP + FN) if (TP + FN) > 0 else 0\n",
        "        fmeasure = f1_score(y_true, y_pred, pos_label=1)\n",
        "        auc = 0\n",
        "        if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "            try:\n",
        "                auc = roc_auc_score(y_true, y_prob[:, 1])\n",
        "            except Exception:\n",
        "                auc = 0\n",
        "\n",
        "    else:\n",
        "        # Multiclass classification\n",
        "        TP = np.diag(cm)\n",
        "        FP = np.sum(cm, axis=0) - TP\n",
        "        FN = np.sum(cm, axis=1) - TP\n",
        "        TN = np.sum(cm) - (FP + FN + TP)\n",
        "\n",
        "        specificity = np.mean([\n",
        "            TN[i] / (TN[i] + FP[i]) if (TN[i] + FP[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        sensitivity = np.mean([\n",
        "            TP[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        gmean = np.sqrt(specificity * sensitivity)\n",
        "        type1 = np.mean([\n",
        "            FP[i] / (FP[i] + TN[i]) if (FP[i] + TN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        type2 = np.mean([\n",
        "            FN[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0 for i in range(num_classes)\n",
        "        ])\n",
        "        fmeasure = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        auc = 0\n",
        "        if y_prob is not None and hasattr(y_prob, \"shape\") and y_prob.shape[1] > 1:\n",
        "            try:\n",
        "                auc = roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')\n",
        "            except Exception:\n",
        "                auc = 0\n",
        "\n",
        "    # Print or return results\n",
        "    print(f\"Accuracy      : {accuracy:.4f}\")\n",
        "    print(f\"Sensitivity   : {sensitivity:.4f}\")\n",
        "    print(f\"Specificity   : {specificity:.4f}\")\n",
        "    print(f\"G-Mean        : {gmean:.4f}\")\n",
        "    print(f\"Type I Error  : {type1:.4f}\")\n",
        "    print(f\"Type II Error : {type2:.4f}\")\n",
        "    print(f\"F1 Score      : {fmeasure:.4f}\")\n",
        "    print(f\"AUROC         : {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZDC1XFkJScb"
      },
      "outputs": [],
      "source": [
        "# --- KNN Variant Models ---\n",
        "def run_knn_variant(name, knn_clf):\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    knn_clf.fit(X_train, y_train)\n",
        "    y_pred = knn_clf.predict(X_test)\n",
        "    if hasattr(knn_clf, \"predict_proba\"):\n",
        "        try:\n",
        "            y_prob = knn_clf.predict_proba(X_test)\n",
        "        except Exception:\n",
        "            y_prob = None\n",
        "    else:\n",
        "        y_prob = None\n",
        "    print_metrics(y_test, y_pred, y_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKAx8jZjLg3E"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('https://raw.githubusercontent.com/muajnstu/ML-Datasets/refs/heads/main/filtered_df.csv')\n",
        "X = df.drop(columns=['Cluster'])\n",
        "y = df['Cluster']\n",
        "\n",
        "#print(\"Class distribution:\\n\", y.value_counts())\n",
        "# --- Handle Imbalanced Data ---\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "#print(\"Balanced class distribution:\\n\", pd.Series(y_resampled).value_counts())\n",
        "# --- Train/Test Split ---\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=46, stratify=y_resampled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PysVYKXh5V3D",
        "outputId": "c8ade4a6-c9b0-4feb-dd8d-341ad75bd0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== KNN ====\n",
            "Accuracy      : 0.9217\n",
            "Sensitivity   : 0.9217\n",
            "Specificity   : 0.9608\n",
            "G-Mean        : 0.9411\n",
            "Type I Error  : 0.0392\n",
            "Type II Error : 0.0783\n",
            "F1 Score      : 0.9205\n",
            "AUROC         : 0.9648\n",
            "\n",
            "==== DistanceKNN ====\n",
            "Accuracy      : 0.9276\n",
            "Sensitivity   : 0.9276\n",
            "Specificity   : 0.9638\n",
            "G-Mean        : 0.9455\n",
            "Type I Error  : 0.0362\n",
            "Type II Error : 0.0724\n",
            "F1 Score      : 0.9264\n",
            "AUROC         : 0.9667\n",
            "\n",
            "==== GeneralizedKNN ====\n",
            "Accuracy      : 0.9268\n",
            "Sensitivity   : 0.9268\n",
            "Specificity   : 0.9634\n",
            "G-Mean        : 0.9449\n",
            "Type I Error  : 0.0366\n",
            "Type II Error : 0.0732\n",
            "F1 Score      : 0.9260\n",
            "AUROC         : 0.9673\n",
            "\n",
            "==== EuclideanKNN ====\n",
            "Accuracy      : 0.9217\n",
            "Sensitivity   : 0.9217\n",
            "Specificity   : 0.9608\n",
            "G-Mean        : 0.9411\n",
            "Type I Error  : 0.0392\n",
            "Type II Error : 0.0783\n",
            "F1 Score      : 0.9205\n",
            "AUROC         : 0.9648\n",
            "\n",
            "==== ManhattanKNN ====\n",
            "Accuracy      : 0.8881\n",
            "Sensitivity   : 0.8881\n",
            "Specificity   : 0.9440\n",
            "G-Mean        : 0.9156\n",
            "Type I Error  : 0.0560\n",
            "Type II Error : 0.1119\n",
            "F1 Score      : 0.8843\n",
            "AUROC         : 0.9506\n",
            "\n",
            "==== ChebyshevKNN ====\n",
            "Accuracy      : 0.8845\n",
            "Sensitivity   : 0.8845\n",
            "Specificity   : 0.9423\n",
            "G-Mean        : 0.9129\n",
            "Type I Error  : 0.0577\n",
            "Type II Error : 0.1155\n",
            "F1 Score      : 0.8853\n",
            "AUROC         : 0.9548\n",
            "\n",
            "==== MahalanobisKNN ====\n",
            "Accuracy      : 0.9620\n",
            "Sensitivity   : 0.9620\n",
            "Specificity   : 0.9810\n",
            "G-Mean        : 0.9715\n",
            "Type I Error  : 0.0190\n",
            "Type II Error : 0.0380\n",
            "F1 Score      : 0.9614\n",
            "AUROC         : 0.9812\n",
            "\n",
            "==== SeuclideanKNN ====\n",
            "Accuracy      : 0.9537\n",
            "Sensitivity   : 0.9537\n",
            "Specificity   : 0.9769\n",
            "G-Mean        : 0.9652\n",
            "Type I Error  : 0.0231\n",
            "Type II Error : 0.0463\n",
            "F1 Score      : 0.9528\n",
            "AUROC         : 0.9791\n",
            "\n",
            "==== WminkowskiKNN ====\n",
            "Accuracy      : 0.9268\n",
            "Sensitivity   : 0.9268\n",
            "Specificity   : 0.9634\n",
            "G-Mean        : 0.9449\n",
            "Type I Error  : 0.0366\n",
            "Type II Error : 0.0732\n",
            "F1 Score      : 0.9260\n",
            "AUROC         : 0.9673\n",
            "\n",
            "Project complete! Check the above output for performance of each KNN variant.\n"
          ]
        }
      ],
      "source": [
        "# Recalculate covariance and variance with the current X_train after SMOTE\n",
        "covariance_matrix = np.cov(X_train.T)\n",
        "stabilized_covariance_matrix = covariance_matrix + np.eye(covariance_matrix.shape[0]) * 1e-6\n",
        "inv_covariance_matrix = np.linalg.inv(stabilized_covariance_matrix)\n",
        "variance_vector = np.var(X_train, axis=0)\n",
        "\n",
        "knn_variants = {\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=3),\n",
        "    \"DistanceKNN\": KNeighborsClassifier(n_neighbors=3, weights='distance'),\n",
        "    \"GeneralizedKNN\": KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3),\n",
        "    \"EuclideanKNN\": KNeighborsClassifier(n_neighbors=3, metric='euclidean'),\n",
        "    \"ManhattanKNN\": KNeighborsClassifier(n_neighbors=3, metric='manhattan'),\n",
        "    \"ChebyshevKNN\": KNeighborsClassifier(n_neighbors=3, metric='chebyshev'),\n",
        "    # Use the recalculated inverse covariance matrix and variance vector\n",
        "    \"MahalanobisKNN\": KNeighborsClassifier(n_neighbors=3, metric='mahalanobis', metric_params={'VI': inv_covariance_matrix}),\n",
        "    \"SeuclideanKNN\": KNeighborsClassifier(n_neighbors=3, metric='seuclidean', metric_params={'V': variance_vector}),\n",
        "    \"WminkowskiKNN\": KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3, metric_params={'w': np.ones(X_train.shape[1])}),\n",
        "    #\"KNNBayes\": KNNBayes(k=3, plot=False),\n",
        "    #\"KNNSVM\": KNNSVM(k=3, plot=False)\n",
        "}\n",
        "\n",
        "\n",
        "for name, model in knn_variants.items():\n",
        "    run_knn_variant(name, model)\n",
        "\n",
        "#print(\"\\n==== KMeansKNN ====\")\n",
        "#kmeansknn = KmeansKNN(n_neighbors=3, output='add')\n",
        "#kmeansknn.fit(X_train, y_train)\n",
        "#y_pred = kmeansknn.predict(X_test)\n",
        "#y_prob = kmeansknn.predict_proba(X_test)\n",
        "#print_metrics(y_test, y_pred, y_prob)\n",
        "\n",
        "print(\"\\nProject complete! Check the above output for performance of each KNN variant.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}